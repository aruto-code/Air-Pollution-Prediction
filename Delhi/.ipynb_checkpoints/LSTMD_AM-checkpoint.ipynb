{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddf239c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arush\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\arush\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "C:\\Users\\arush\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas_datareader as pdr\n",
    "from datetime import datetime\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42f2f85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.engine.sequential import Sequential\n",
    "\n",
    "from tensorflow.python.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.layers import LSTM ,Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import metrics\n",
    "from keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c805c41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('A:\\OverallDataset\\Delhi\\OverallDelhi.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baa76f5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>From Date</th>\n",
       "      <th>To Date</th>\n",
       "      <th>PM2.5 (ug/m3)</th>\n",
       "      <th>PM10 (ug/m3)</th>\n",
       "      <th>NOx (ppb)</th>\n",
       "      <th>SO2 (ug/m3)</th>\n",
       "      <th>CO (mg/m3)</th>\n",
       "      <th>Ozone (ug/m3)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-01-2010 00:00</td>\n",
       "      <td>01-01-2010 01:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.75</td>\n",
       "      <td>4.27</td>\n",
       "      <td>4.43</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01-01-2010 01:00</td>\n",
       "      <td>01-01-2010 02:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.25</td>\n",
       "      <td>4.55</td>\n",
       "      <td>3.69</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01-01-2010 02:00</td>\n",
       "      <td>01-01-2010 03:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.25</td>\n",
       "      <td>4.62</td>\n",
       "      <td>3.68</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01-01-2010 03:00</td>\n",
       "      <td>01-01-2010 04:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.25</td>\n",
       "      <td>4.52</td>\n",
       "      <td>3.11</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01-01-2010 04:00</td>\n",
       "      <td>01-01-2010 05:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.25</td>\n",
       "      <td>4.70</td>\n",
       "      <td>2.80</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048570</th>\n",
       "      <td>02-04-2012 11:00</td>\n",
       "      <td>02-04-2012 12:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048571</th>\n",
       "      <td>02-04-2012 12:00</td>\n",
       "      <td>02-04-2012 13:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048572</th>\n",
       "      <td>02-04-2012 13:00</td>\n",
       "      <td>02-04-2012 14:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048573</th>\n",
       "      <td>02-04-2012 14:00</td>\n",
       "      <td>02-04-2012 15:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048574</th>\n",
       "      <td>02-04-2012 15:00</td>\n",
       "      <td>02-04-2012 16:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1048575 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                From Date           To Date  PM2.5 (ug/m3)  PM10 (ug/m3)  \\\n",
       "0        01-01-2010 00:00  01-01-2010 01:00            NaN           NaN   \n",
       "1        01-01-2010 01:00  01-01-2010 02:00            NaN           NaN   \n",
       "2        01-01-2010 02:00  01-01-2010 03:00            NaN           NaN   \n",
       "3        01-01-2010 03:00  01-01-2010 04:00            NaN           NaN   \n",
       "4        01-01-2010 04:00  01-01-2010 05:00            NaN           NaN   \n",
       "...                   ...               ...            ...           ...   \n",
       "1048570  02-04-2012 11:00  02-04-2012 12:00            NaN           NaN   \n",
       "1048571  02-04-2012 12:00  02-04-2012 13:00            NaN           NaN   \n",
       "1048572  02-04-2012 13:00  02-04-2012 14:00            NaN           NaN   \n",
       "1048573  02-04-2012 14:00  02-04-2012 15:00            NaN           NaN   \n",
       "1048574  02-04-2012 15:00  02-04-2012 16:00            NaN           NaN   \n",
       "\n",
       "         NOx (ppb)  SO2 (ug/m3)  CO (mg/m3)  Ozone (ug/m3)  \n",
       "0            38.75         4.27        4.43            3.0  \n",
       "1            23.25         4.55        3.69            3.5  \n",
       "2            23.25         4.62        3.68            3.5  \n",
       "3            18.25         4.52        3.11            4.0  \n",
       "4            16.25         4.70        2.80            4.0  \n",
       "...            ...          ...         ...            ...  \n",
       "1048570        NaN          NaN         NaN            NaN  \n",
       "1048571        NaN          NaN         NaN            NaN  \n",
       "1048572        NaN          NaN         NaN            NaN  \n",
       "1048573        NaN          NaN         NaN            NaN  \n",
       "1048574        NaN          NaN         NaN            NaN  \n",
       "\n",
       "[1048575 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3812d73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'From Date' contains string values.\n",
      "Column 'To Date' contains string values.\n",
      "Column 'PM2.5 (ug/m3)' does not contain string values.\n",
      "Column 'PM10 (ug/m3)' does not contain string values.\n",
      "Column 'NOx (ppb)' does not contain string values.\n",
      "Column 'SO2 (ug/m3)' does not contain string values.\n",
      "Column 'CO (mg/m3)' does not contain string values.\n",
      "Column 'Ozone (ug/m3)' does not contain string values.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Function to check if a column contains string values\n",
    "def contains_strings(column):\n",
    "    return any(isinstance(value, str) for value in column)\n",
    "\n",
    "# Iterate through the columns and check for string values\n",
    "for column in df.columns:\n",
    "    if contains_strings(df[column]):\n",
    "        print(f\"Column '{column}' contains string values.\")\n",
    "    else:\n",
    "        print(f\"Column '{column}' does not contain string values.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e989562b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your DataFrame is named 'df'\n",
    "df['From Date'] = pd.to_datetime(df['From Date'], format=\"%d-%m-%Y %H:%M\")\n",
    "df['To Date'] = pd.to_datetime(df['To Date'], format=\"%d-%m-%Y %H:%M\")\n",
    "\n",
    "# Verify the data types of the columns\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaec8074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Check for missing values in 'PM2.5 (ug/m3)' and 'PM10 (ug/m3)' columns\n",
    "missing_pm25 = df['PM2.5 (ug/m3)'].isnull().sum()\n",
    "missing_pm10 = df['PM10 (ug/m3)'].isnull().sum()\n",
    "\n",
    "if missing_pm25 > 0:\n",
    "    print(f\"Number of missing values in 'PM2.5 (ug/m3)': {missing_pm25}\")\n",
    "\n",
    "if missing_pm10 > 0:\n",
    "    print(f\"Number of missing values in 'PM10 (ug/m3)': {missing_pm10}\")\n",
    "\n",
    "# Check for outliers in 'PM2.5 (ug/m3)' and 'PM10 (ug/m3)' columns\n",
    "z_scores_pm25 = np.abs((df['PM2.5 (ug/m3)'] - df['PM2.5 (ug/m3)'].mean()) / df['PM2.5 (ug/m3)'].std())\n",
    "z_scores_pm10 = np.abs((df['PM10 (ug/m3)'] - df['PM10 (ug/m3)'].mean()) / df['PM10 (ug/m3)'].std())\n",
    "\n",
    "outliers_pm25 = (z_scores_pm25 > 3)\n",
    "outliers_pm10 = (z_scores_pm10 > 3)\n",
    "\n",
    "if outliers_pm25.any():\n",
    "    print(f\"Outliers detected in 'PM2.5 (ug/m3)':\")\n",
    "    print(df[outliers_pm25])\n",
    "\n",
    "if outliers_pm10.any():\n",
    "    print(f\"Outliers detected in 'PM10 (ug/m3)':\")\n",
    "    print(df[outliers_pm10])\n",
    "\n",
    "# Handle missing values and outliers as needed\n",
    "# You can choose to remove or impute missing values, and handle outliers according to your data analysis requirements.\n",
    "\n",
    "# After handling missing values and outliers, you can proceed with AQI calculations as shown in previous responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301504c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values in 'PM2.5 (ug/m3)' and 'PM10 (ug/m3)' columns\n",
    "df = df.dropna(subset=['PM2.5 (ug/m3)', 'PM10 (ug/m3)'])\n",
    "\n",
    "# Calculate z-scores for 'PM2.5 (ug/m3)' and 'PM10 (ug/m3)' columns\n",
    "z_scores_pm25 = np.abs((df['PM2.5 (ug/m3)'] - df['PM2.5 (ug/m3)'].mean()) / df['PM2.5 (ug/m3)'].std())\n",
    "z_scores_pm10 = np.abs((df['PM10 (ug/m3)'] - df['PM10 (ug/m3)'].mean()) / df['PM10 (ug/m3)'].std())\n",
    "\n",
    "# Define a threshold for outliers (e.g., 3 standard deviations)\n",
    "outlier_threshold = 3\n",
    "\n",
    "# Drop rows with outliers in 'PM2.5 (ug/m3)' and 'PM10 (ug/m3)' columns\n",
    "df = df[~(z_scores_pm25 > outlier_threshold)]\n",
    "df = df[~(z_scores_pm10 > outlier_threshold)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d50b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['PM2.5 (ug/m3)'].dtype)\n",
    "print(df['PM10 (ug/m3)'].dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c3059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa07210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming the actual column names in your dataset\n",
    "pm25_column = 'PM2.5 (ug/m3)'\n",
    "pm10_column = 'PM10 (ug/m3)'\n",
    "\n",
    "# Constants for AQI calculation (Indian standard)\n",
    "pm25_breakpoints = [0, 30, 60, 90, 120, 250]\n",
    "pm25_aqi_values = [0, 50, 100, 200, 300, 400, 500]\n",
    "\n",
    "pm10_breakpoints = [0, 50, 100, 250, 350, 430]\n",
    "pm10_aqi_values = [0, 50, 100, 200, 300, 400, 500]\n",
    "\n",
    "# Calculate AQI for PM2.5\n",
    "def calculate_aqi_pm25(pm25):\n",
    "    for i in range(len(pm25_breakpoints) - 1):\n",
    "        if pm25_breakpoints[i] <= pm25 <= pm25_breakpoints[i + 1]:\n",
    "            aqi = ((pm25_aqi_values[i + 1] - pm25_aqi_values[i]) / (pm25_breakpoints[i + 1] - pm25_breakpoints[i])) * (pm25 - pm25_breakpoints[i]) + pm25_aqi_values[i]\n",
    "            return aqi\n",
    "    return pm25_aqi_values[-1]\n",
    "\n",
    "# Calculate AQI for PM10\n",
    "def calculate_aqi_pm10(pm10):\n",
    "    for i in range(len(pm10_breakpoints) - 1):\n",
    "        if pm10_breakpoints[i] <= pm10 <= pm10_breakpoints[i + 1]:\n",
    "            aqi = ((pm10_aqi_values[i + 1] - pm10_aqi_values[i]) / (pm10_breakpoints[i + 1] - pm10_breakpoints[i])) * (pm10 - pm10_breakpoints[i]) + pm10_aqi_values[i]\n",
    "            return aqi\n",
    "    return pm10_aqi_values[-1]\n",
    "\n",
    "# Calculate AQI for each row in the dataset\n",
    "def calculate_aqi(row):\n",
    "    aqi_pm25 = calculate_aqi_pm25(row[pm25_column])\n",
    "    aqi_pm10 = calculate_aqi_pm10(row[pm10_column])\n",
    "    return max(aqi_pm25, aqi_pm10)\n",
    "\n",
    "# Apply the calculate_aqi function to your dataset\n",
    "df['AQI'] = df.apply(calculate_aqi, axis=1)\n",
    "\n",
    "# Now, your dataset should have an 'AQI' column with the calculated Air Quality Index.\n",
    "\n",
    "# You can further save this dataset or use it for analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f78290",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02bd24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = np.nan_to_num(X_train)  # Replace NaN with 0\n",
    "#X_test = np.nan_to_num(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f45257e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#scaler = StandardScaler()\n",
    "#X_train = scaler.fit_transform(X_train)\n",
    "#X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c75ae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Check for NaN values in the entire DataFrame\n",
    "nan_df = df.isna()\n",
    "\n",
    "# Check for NaN values in a specific column\n",
    "nan_in_column = df['AQI'].isna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ccd968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling all the null values with mean.\n",
    "columns_to_fill = [\n",
    "    'PM2.5 (ug/m3)',\n",
    "    'PM10 (ug/m3)',\n",
    "    'NOx (ppb)',\n",
    "    'SO2 (ug/m3)',\n",
    "    'CO (mg/m3)',\n",
    "    'Ozone (ug/m3)',\n",
    "]\n",
    "\n",
    "# Fill null values in the specified columns with their respective means\n",
    "df[columns_to_fill] = df[columns_to_fill].fillna(df[columns_to_fill].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f975810",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac638dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for infinite values in the entire DataFrame\n",
    "# is_infinite = np.isinf(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37168108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify rows with infinite values\n",
    "# rows_with_infinity = df[is_infinite.any(axis=1)]\n",
    "\n",
    "# # Identify columns with infinite values\n",
    "# columns_with_infinity = df.columns[is_infinite.any()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6600cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Rows with infinity:\")\n",
    "# print(rows_with_infinity)\n",
    "\n",
    "# print(\"Columns with infinity:\")\n",
    "# print(columns_with_infinity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8ed65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# List of columns to check for infinity\n",
    "columns_to_check = ['PM2.5 (ug/m3)', 'PM10 (ug/m3)',  'NOx (ppb)',\n",
    "                     'SO2 (ug/m3)', 'CO (mg/m3)', 'Ozone (ug/m3)', 'AQI']\n",
    "\n",
    "# Loop through the specified columns and print rows with infinity values\n",
    "for column_name in columns_to_check:\n",
    "    # Print rows where the specific column contains positive infinity\n",
    "    rows_with_infinity = df[df[column_name] == np.inf]\n",
    "\n",
    "    print(\"Rows with positive infinity in column {}: \".format(column_name))\n",
    "    print(rows_with_infinity)\n",
    "\n",
    "    # Print rows where the specific column contains negative infinity\n",
    "    rows_with_neg_infinity = df[df[column_name] == -np.inf]\n",
    "\n",
    "    print(\"Rows with negative infinity in column {}: \".format(column_name))\n",
    "    print(rows_with_neg_infinity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55d9a9d-3185-42d1-8318-cb99bc446611",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cbf640-1d8a-4696-b258-95c4ec32e225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "# Convert 'From Date' and 'To Date' to timestamps\n",
    "df['From Date'] = pd.to_datetime(df['From Date'])\n",
    "df['To Date'] = pd.to_datetime(df['To Date'])\n",
    "\n",
    "# Extract relevant features from the timestamp\n",
    "df['Year'] = df['From Date'].dt.year\n",
    "df['Month'] = df['From Date'].dt.month\n",
    "# Add more features as needed (e.g., day, hour, minute)\n",
    "\n",
    "# Define your target variable and features\n",
    "y = df['AQI']  # Assuming 'AQI' is the column with AQI values\n",
    "X = df.drop(columns=['AQI', 'From Date', 'To Date'])  # Exclude the AQI and date columns\n",
    "\n",
    "# Normalize your features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split your dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape the data for LSTM input\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Define a custom attention layer\n",
    "class AttentionLayer(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W_q = self.add_weight(name=\"W_q\", shape=(input_shape[-1], input_shape[-1]), initializer=\"uniform\")\n",
    "        self.W_k = self.add_weight(name=\"W_k\", shape=(input_shape[-1], input_shape[-1]), initializer=\"uniform\")\n",
    "        self.b_q = self.add_weight(name=\"b_q\", shape=(input_shape[-1],), initializer=\"zeros\")\n",
    "        self.b_k = self.add_weight(name=\"b_k\", shape=(input_shape[-1],), initializer=\"zeros\")\n",
    "\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        q = tf.nn.tanh(tf.matmul(inputs, self.W_q) + self.b_q)\n",
    "        k = tf.nn.tanh(tf.matmul(inputs, self.W_k) + self.b_k)\n",
    "\n",
    "        attention = tf.reduce_sum(q * k, axis=-1, keepdims=True)\n",
    "        attention = tf.nn.softmax(attention, axis=1)\n",
    "        output = inputs * attention\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "# Define the LSTM model with attention mechanism\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(units=64, return_sequences=True, input_shape=(X_train.shape[1], 1)),\n",
    "    AttentionLayer(),\n",
    "    tf.keras.layers.LSTM(units=64, return_sequences=True),\n",
    "    AttentionLayer(),\n",
    "    tf.keras.layers.LSTM(units=64),\n",
    "    tf.keras.layers.Dense(1)  # Output layer for AQI prediction\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=13, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Make predictions on training and testing data\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate performance metrics for training data\n",
    "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# Calculate performance metrics for testing data\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Print the performance metrics\n",
    "print(\"Training Metrics:\")\n",
    "print(\"MAE: \", mae_train)\n",
    "print(\"RMSE: \", rmse_train)\n",
    "print(\"R2: \", r2_train)\n",
    "\n",
    "print(\"\\nTesting Metrics:\")\n",
    "print(\"MAE: \", mae_test)\n",
    "print(\"RMSE: \", rmse_test)\n",
    "print(\"R2: \", r2_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
